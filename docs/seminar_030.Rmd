---
title: 'Семинар 3. Кластерный анализ'
date: 'May 31, 2018'
output:
  html_document:
    keep_md: no
    number_sections: yes
    toc: yes
lang: ru-RU
editor_options:
  chunk_output_type: console
---


```{r}
library(tidyverse) # обработка данных, графики...
library(skimr)# описательные статистики
library(rio) # импорт фантастического количества форматов данных
library(cluster) # кластерный анализ
library(factoextra) # визуализации kmeans, pca,
library(broom) # стандартизация информации о модели
library(dendextend) # визуализация дендограмм
```

# Кластеризация k-means

Для примера возьмём данные по потреблению протеинов Европе из книги [Practial Machine Learning Cookbook](https://github.com/PacktPublishing/Practical-Machine-Learning-Cookbook/blob/master/Chapter%2003/Data/Europenaprotein.csv).
Сначала, как всегда, загрузим их и посмотрим описательные статистики.

```{r}
df <- import('Europenaprotein.csv')
skim(df)
```

- Есть ли в данных пропущенные наблюдения?

Затем отмасштабируем данные с помощью встроенной функции `scale()`.
Поскольку она может работать только с числами, первый столбец `Country` ей передавать не нужно.
Результат сохраним в матрице `df.stand` и сделаем из неё `data.frame`.

- Посмотрите на описательные статистики. Что стало со средним значением и стандартным отклонением переменных?

```{r}
mx_stand2 <- mutate_all(df, -Country, fun = list(scale))
mx.stand <- scale(df %>% select(-scale))
df.stand <- data.frame(mx.stand)

x2 <- mutate_if(x, is.numeric, scale)

rownames(df.stand) = df$Country
skim(df.stand)
```

* Упражнение 1.

- Посмотрите на описательные статистики встроенного набора данных об арестах в США.
- Отмасштабируйте данные и сохраните их в перменную `usa.stand`.
- Проверьте, что всё получилось :)

```{r}
usa <- USArrests
skim(___)

usa.stand <- ___
___
```

Выполним кластеризацию методом k-средних с помощью функции `kmenas`.
В качестве аргументов укажем отмасштабированные данные `x` и количество кластеров `centers`.
Пока мы не знаем, как выбирать оптимальное количество кластеров, поэтому предположим, что их три.
Сохраним результат этого действия в переменной `k.means.fit`.

```{r}
k.means.fit <- kmeans(x = df.stand, centers = 3)
k.means.fit
# здесь график!
```

* Упражнение 2.

Кластеризуйте отмасштабированные данные по арестам в США.
В выборе числа кластеров доверьтесь интуиции.

```{r}
k.means.usa <- kmeans(___, centers = ___)
```

В переменной `k.means.fit` лежит куча разной информации!
Выведем список доступных блоков информации, которые возвращает функция `kmeans`, командой `attributes()`.
Теперь, зная, что искать, мы можем посмотреть, например, на координаты центра кластеров или количество объектов в каждом из них.

```{r}
attributes(k.means.fit)

k.means.fit$centers
k.means.fit$cluster
k.means.fit$size
```

Другой способ структурировать вывод `kmeans` — использовать команду `tidy` из пакета `broom`.

```{r}
tidy(k.means.fit)
```

Первые девять неназванных переменных — центры кластеров по каждой переменной.

<!--
TODO: ещё в самом RStudio можно посмотреть
-->

Осталось только визуализировать результаты!
Для этого будем использовать команду `fviz_cluster()` из пакета `factoextra`.
Её аргументы — результат кластеризации (в нашем случае — `k.means.fit`),
исходные данные и ещё куча настроек вроде размера точек и цвета наблюдений-выбросов.
Мы только попросим выделять цевтом кластеры по их границам и укажем аргумент `ellipse.type = 'convex'`.

```{r}
fviz_cluster(object = k.means.fit, data = df.stand,
             ellipse.type = 'convex')
```

* Упражнение ???.

Визуализируйте результаты кластеризации.

```{r}
fviz_cluster(___, data = ___, ellipse.type = 'convex')
```

<!--
TODO: визулизация корреляций
-->

Как понять, сколько кластеров брать оптимально?
Один из способов сделать это — воспользоваться командой `fviz_nbclust` из пакета `factoextra`.

```{r}
fviz_nbclust(df.stand, kmeans, method = 'wss') +
  labs(subtitle = 'Elbow method')
fviz_nbclust(df.stand, kmeans, method = 'silhouette') +
  labs(subtitle = 'Silhouette method')
fviz_nbclust(df.stand, kmeans, method = 'gap_stat') +
  labs(subtitle = 'Gap statistic method')
```

<!--
TODO: объединение диаграмм в одну https://github.com/thomasp85/patchwork
-->

* Упражнение ???.

<!--
TODO: дописать код для упражнения
-->

Проверьте, совпадает ли выбранное вами число кластеров с оптимальным.
(?) Изобразите все три диаграммы вместе.

```{r}
___
```


# Иерархическая кластеризация

Другой способ разбить данные на группы — иерархическая кластеризация.
Но, в отличие от метода k-средних, она работает с матрицей расстояний,
поэтому первым делом посчитаем её!
Для этого будем использовать функцию `dist()`.
Передадим ей стандартизированные данные и укажем явно, как считать расстояния с помощью аргумента `method`.
О всех остальных опциях можно узнать в справке.

```{r}
df.dist <- dist(x = df.stand, method = 'euclidian')
```

Расстояния тоже можно визуализировать!
Сделаем это командой `fviz_dist` из пакета `factoextra`.

```{r}
fviz_dist(df.dist)
```

* Упражнение ???.

Посчитайте матрицу расстояний для `usa.stand` и визуализируйте её.

```{r}
usa.dist <- dist(___, method = 'euclidian')
fviz_dist(___)
```

Затем полученную матрицу расстояний передадим функции `hclust()`, которая и кластеризует данные.

```{r}
h.fit <- hclust(df.dist, method = 'ward.D2')
```

С помощью функции `fviz_dend` визуализируем результат кластеризации.
Укажем несколько аргументов, чтобы сделать дендограмму красивее,
а полный перечень найдётся в справке.

```{r}
fviz_dend(h.fit, k = 4, # количесвто групп
          cex = 0.5, # размер подписи
          color_labels_by_k = TRUE) # цвет подписей по группам
```

* Упражнение ???.

- Сделайте иерархичискую кластеризацию на данных об арестах.
Не забудьте, что функции `hclust()` мы передаём матрицу расстояний!

- Визуализируйте результат кластеризации.
Выделите на графике четыре группы и сделайте подписи цветными.

```{r}
usa.fit <- hclust(___, method = 'ward.D2')
fviz_dend(___, k = ___, # количесвто групп
          cex = 0.5, # размер подписи
          color_labels_by_k = ___) # цвет подписей по группам
```

Ура :)
