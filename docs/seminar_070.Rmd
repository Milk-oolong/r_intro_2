---
title: 'Семинар 7. Классификация'
date: 'Июнь, 19, 2018'
output:
  html_document:
    keep_md: no
    number_sections: yes
    toc: yes
lang: ru-RU
editor_options:
  chunk_output_type: console
---



Шаманское заклинание для настройки глобальных опций отчёта:
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r}
library(tidyverse) # обработка данных, графики...
library(skimr) # описательные статистики
library(rio) # импорт фантастического количества форматов данных
library(broom) # метла превращает результаты оценивания моделей в таблички
library(GGally) # больше готовых графиков
library(sjPlot) # ещё больше графиков
library(lmtest) # диагностика линейных моделей
library(Ecdat) # много-много разных наборов данных
library(sjstats) # удобные мелкие функции для работы с моделями
library(sandwich) # оценка Var для гетероскедастичности
library(caret) # пакет для подбора параметров разных моделей
library(margins) # для подсчёта предельных эффектов
library(rpart.plot) # для картинок деревьев
```


Импортируем набор данных по успеваемости студентов и смотрим на него :)

```{r}
educ <- import('xAPI-Edu-Data.csv')
skim(educ)
```

Данные взяты с [kaggle.com](https://www.kaggle.com/aljarah/xAPI-Edu-Data/).
Целевая переменная — успеваемость студента, `Class`, принимает три значения, 'H' (high), 'M' (mid), 'L' (low).
Остальные переменные — характеристики студента.


Для целей бинарной классификации объединяем две верхних категории в одну:
```{r}
educ_logit <- mutate(educ, y = fct_collapse(Class, H = c('M', 'H')))
```

И объявляем все текстовые переменные факторными:
```{r}
educ_fct <- mutate_if(educ, is.character, factor)
```

Разделим выборку на две части, обучающую и тестовую.
По тестовой мы сможем оценить качество прогнозов нашей модели.

Разбиение выборки на две части — это случаная операция,
поэтому зададим на удачу зерно генератора случайных чисел.

Создадим вектор `train_rows` с номерами строк для обучающей части.
```{r}
set.seed(777)
train_rows <- createDataPartition(educ_fct$y, p = 0.8, list = FALSE)
```

И разделим выборку согласно вектору `train_rows`:
```{r}
educ_train <- educ_fct[train_rows, ]
educ_test <- educ_fct[-train_rows, ]
```


# Логистическая регрессия


Реализуем логистическую регрессию с помощью функции `glm`.
Передадим ей набор данных `educ_train`, формулу и укажем специалный аргумент `family = binomial(link = 'logit')`.
Сохраним результаты оценивания в переменную `educ_lmodel` и посмотрим на них.

```{r}
educ_lmodel <- train(data = educ_train, y ~ gender + SectionID + raisedhands, family = binomial(link = 'logit'), method = 'glm')
summary(educ_lmodel)
```

TODO: вопросы

Чтобы найти предельные эффекты, воспользуемся функцией `margins` из одноимённого пакета.
Снова выведем результаты командой `summary()`.

```{r}
educ_margins <- margins(educ_lmodel$finalModel)
summary(educ_margins)
```

TODO: вопросы

Их тоже можно визуализировать!
Эта функция реализована в пакете `ggeffects` и называетя `ggpredict`.
Ей нужно передать оценённую модель и указать переменные для визуализации в аргументе `terms`.

```{r}
pred_vis <- ggpredict(educ_lmodel$finalModel, terms = c('raisedhands', 'gender'))
plot(pred_vis)
```

Посторим прогнозы модели для тестовых данных `educ_test`.
Для этого будем использовать функцию `predict`, которой передадим оценённую модель `educ_lmodel`.
В переменной `educ_predz` уже будут лежать предсказанные классы.
Чтобы получить их вероятности, нужно добавить аргумент `type = 'prob'`.

```{r}
educ_pred <- predict(educ_lmodel, newdata = educ_test)
head(educ_pred)

educ_prob <- predict(educ_lmodel, newdata = educ_test, type = 'prob')
head(educ_prob)
```

Теперь мы можем посмотреть на матрицу ошибок и получить узнать, насколько хорошую модель мы оецнили.
Для этого будем использовть функцию `confusionMatrix` из пакета `caret`.
В качестве аргумента `data` нужно указать предсказанные значения, а в `reference` — правильные ответы.

```{r}
confusionMatrix(data = educ_pred, reference = educ_test$y)
```

TODO: доделаю про ROC





# На природу, к деревьям и в лес!

Бинарные деревья очень легко интепретируются при приемлемом качестве прогнозов.

Алгоритмы построения дерева отличаются деталями:

- по какому критерию делить веточку на две?
- когда остановить процесс деления веточек?
- следует ли обрезать дерево после окончания деления?
- как обрабатывать пропущенные значений?

```{r}
tree_model <- train(y ~ . - Class, data = educ_test,
                      method = "rpart2",
                      na.action = na.omit)
```

Картинка дерева:
```{r}
rpart.plot(tree_model$finalModel)
```

Описание дерева:
```{r}
summary(tree_model)
```

Предсказываем с помощью дерева на тестовой выборке:
```{r}
educ_tree <- mutate(educ_test,
  yhat = predict(tree_model, educ_test, na.action = na.pass))
confusionMatrix(educ_tree$yhat, educ_tree$y)
```

Пакет `caret` представляет собой общий интерфейс ко многим моделям,
однако некоторые модели он ещё не поддерживает.
Например, быстрые и стройные, [fast and frugal](https://cran.r-project.org/web/packages/FFTrees/vignettes/FFTrees_heart.html), деревья.

Быстрые деревья нужны, например, в медицине, чтобы
создавать быстрые и простые рекомендации для спасения жизни.

Мы применим их для спасения неуспевающих студентов :)
Увы, пакет принимает на вход только 0/1 в зависимой переменной, поэтому заменим названия категорий на числа:
```{r}
educ_train2 <- mutate(educ_train, ybin = ifelse(y == 'H', 1, 0)) %>%
select(-Class)
```

И построим картинку для быстрого и стройного дерева:
```{r}
fftree_model <- FFTrees(formula = ybin ~ .,
                     data = educ_train2)
plot(fftree_model)

```



# Случайный лес

В алгоритме случайного леса мы

1. Выращиваем целый лес, скажем 500, деревьев.

2. Строим прогноз с помощью каждого дерева.

3. Агрегируем прогнозы деревьев. Можно в качестве итогового прогноза выбрать ту категорию, за которую проголосовало большинство деревьев. Можно оценить вероятности категорий, взяв процент деревьев, проголосовавших за ту или иную категорию.

Деревья оказываются не идеальными копиями друг друга по двум причинам:

1. Каждое дерево обучается на случайной выборке из исходной выборки. Обычно для каждого дерева берут подвыборку с повторениями из исходной выборки, так чтобы размер подвыборки равнялся размеру исходной выборки.

2. При каждом делении каждой ветки на две части происходит предварительный случайный отбор переменных. Скажем, из исходных 100 переменных, каждый раз случайно отбирается 10, а затем из этих 10 выбирается наилучшая, по которой ветвь и делится на две ветви.

У идеи есть куча вариантов исполнения, отличающихся деталями:

- критерием деления ветви на две;
- критерием остановки деления дерева;
- количеством предварительно отбираемых переменных перед каждым делением;
- количество деревьев;


Посмотрим на все вариации случайного леса, которые перебрал `ranger`
```{r}
ranger_model <- train(y ~ . - Class, data = educ_test,
                    method = "ranger",
                    na.action = na.omit,
                    importance = 'impurity')
ranger_model
plot(ranger_model)
```

И более подробно про наилучшую:
```{r}
ranger_model$finalModel
```


К сожалению, построить информативно про все сотни деревьев невозможно.

Можно попытаться выделить важность переменных
```{r}
ranger_import <- varImp(ranger_model)
ranger_import
plot(ranger_import)
```


И, конечно, построить прогнозы:
```{r}
educ_ranger <- mutate(educ_test,
  yhat = predict(ranger_model, educ_test, na.action = na.pass))
confusionMatrix(educ_ranger$yhat, educ_ranger$y)
```

По умолчанию, пакет `caret` сам решает, сколько значений параметров перебирать и какие конкретно.

Список перебираемых параметров:
```{r}
modelLookup(model = 'ranger')
```


Но мы можем заказать перебор любых.

Можно заказать количество перебираемых значений
```

Или явно значения



Ура :)
