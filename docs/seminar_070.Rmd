---
title: 'Семинар 7. Классификация'
date: 'Июнь, 19, 2018'
output:
  html_document:
    keep_md: no
    number_sections: yes
    toc: yes
lang: ru-RU
editor_options:
  chunk_output_type: console
---



Шаманское заклинание для настройки глобальных опций отчёта:
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r}
library(tidyverse) # обработка данных, графики...
library(skimr) # описательные статистики
library(rio) # импорт фантастического количества форматов данных
library(broom) # метла превращает результаты оценивания моделей в таблички
library(GGally) # больше готовых графиков
library(sjPlot) # ещё больше графиков
library(lmtest) # диагностика линейных моделей
library(Ecdat) # много-много разных наборов данных
library(sjstats) # удобные мелкие функции для работы с моделями
library(sandwich) # оценка Var для гетероскедастичности
library(caret) # пакет для подбора параметров разных моделей
library(margins) # для подсчёта предельных эффектов
```


Импортируем набор данных по успеваемости студентов и смотрим на него :)

```{r}
educ <- import('xAPI-Edu-Data.csv')
skim(educ)
```

Данные взяты с [kaggle.com](https://www.kaggle.com/aljarah/xAPI-Edu-Data/).
Целевая переменная — успеваемость студента, `Class`, принимает три значения, 'H' (high), 'M' (mid), 'L' (low).
Остальные переменные — характеристики студента.


Для целей бинарной классификации объединяем две верхних категории в одну:
```{r}
educ_logit <- mutate(educ, y = fct_collapse(Class, H = c('M', 'H')))
```

И объявляем все текстовые переменные факторными:
```{r}
educ_fct <- mutate_if(educ, is.character, factor)
```

Разделим выборку на две части, обучающую и тестовую.
По тестовой мы сможем оценить качество прогнозов нашей модели.

Разбиение выборки на две части — это случаная операция,
поэтому зададим на удачу зерно генератора случайных чисел.

Создадим вектор `train_rows` с номерами строк для обучающей части.
```{r}
set.seed(777)
train_rows <- createDataPartition(educ_fct$y, p = 0.8, list = FALSE)
```

И разделим выборку согласно вектору `train_rows`:
```{r}
educ_train <- educ_fct[train_rows, ]
educ_test <- educ_fct[-train_rows, ]
```


# Логистическая регрессия




# На природу, к деревьям и в лес!

Бинарные деревья очень легко интепретируются при приемлемом качестве прогнозов.

Алгоритмы построения дерева отличаются деталями:

- по какому критерию делить веточку на две?
- когда остановить процесс деления веточек?
- следует ли обрезать дерево после окончания деления?
- как обрабатывать пропущенные значений?

```{r}
tree_model <- train(y ~ . - Class, data = educ_test,
                      method = "rpart2",
                      na.action = na.omit)
```

Картинка дерева:
```{r}
rpart.plot(tree_model$finalModel)
```

Описание дерева:
```{r}
summary(tree_model)
```

Предсказываем с помощью дерева на тестовой выборке:
```{r}
tree_predictions <- predict(tree_model, educ_test, na.action = na.pass)
tree_report <- confusionMatrix(tree_predictions, educ_test$y)
tree_report
```



# Случайный лес

В алгоритме случайного леса мы

1. Выращиваем целый лес, скажем 500, деревьев.

2. Строим прогноз с помощью каждого дерева.

3. Агрегируем прогнозы деревьев. Можно в качестве итогового прогноза выбрать ту категорию, за которую проголосовало большинство деревьев. Можно оценить вероятности категорий, взяв процент деревьев, проголосовавших за ту или иную категорию.

Деревья оказываются не идеальными копиями друг друга по двум причинам:

1. Каждое дерево обучается на случайной выборке из исходной выборки. Обычно для каждого дерева берут подвыборку с повторениями из исходной выборки, так чтобы размер подвыборки равнялся размеру исходной выборки.

2. При каждом делении каждой ветки на две части происходит предварительный случайный отбор переменных. Скажем, из исходных 100 переменных, каждый раз случайно отбирается 10, а затем из этих 10 выбирается наилучшая, по которой ветвь и делится на две ветви.

У идеи есть куча вариантов исполнения, отличающихся деталями:

- критерием деления ветви на две;
- критерием остановки деления дерева;
- количеством предварительно отбираемых переменных перед каждым делением;
- количество деревьев;


```{r}


```
